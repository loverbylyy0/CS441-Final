{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyMXIla4PN5wGfqwhAbVZipP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# CS 441 final project"],"metadata":{"id":"Bix_Ml0nfaRb"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","from PIL import Image\n","import time\n","import torch\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision import transforms, models\n","\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.svm import LinearSVC\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.decomposition import PCA\n","\n","import torch.nn as nn\n","import torch.optim as optim\n","DRIVE_BASE = \"/content/drive/My Drive/CS441/Final/garbage_dataset\"\n","LOCAL_TRAIN = \"/content/garbage_train\"\n","LOCAL_TEST  = \"/content/garbage_test\"\n","\n","FORCE_SYNC = False\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","!pip -q install pillow-heif\n","from PIL import Image\n","import pillow_heif\n","pillow_heif.register_heif_opener()\n","\n","import os, subprocess\n","\n","IMG_EXTS = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\", \".heic\", \".heif\")\n","\n","def count_images(root):\n","    if not os.path.exists(root):\n","        return 0\n","    c = 0\n","    for _, _, files in os.walk(root):\n","        for f in files:\n","            if f.lower().endswith(IMG_EXTS):\n","                c += 1\n","    return c\n","\n","def rsync_dir(src, dst):\n","    cmd = [\"rsync\", \"-a\", \"--delete\", \"--info=progress2\", src.rstrip(\"/\") + \"/\", dst.rstrip(\"/\") + \"/\"]\n","    print(\"Running:\", \" \".join([f'\"{x}\"' if \" \" in x else x for x in cmd]))\n","    subprocess.run(cmd, check=True)\n","\n","drive_train = os.path.join(DRIVE_BASE, \"train\")\n","drive_test  = os.path.join(DRIVE_BASE, \"test\")\n","\n","if not os.path.exists(drive_train):\n","    raise FileNotFoundError(f\"Not found: {drive_train}\")\n","if not os.path.exists(drive_test):\n","    raise FileNotFoundError(f\"Not found: {drive_test}\")\n","\n","drive_train_cnt = count_images(drive_train)\n","drive_test_cnt  = count_images(drive_test)\n","\n","local_train_cnt = count_images(LOCAL_TRAIN)\n","local_test_cnt  = count_images(LOCAL_TEST)\n","\n","print(f\"Drive train images: {drive_train_cnt} | Local train images: {local_train_cnt}\")\n","print(f\"Drive test  images: {drive_test_cnt}  | Local test  images: {local_test_cnt}\")\n","\n","need_sync = FORCE_SYNC or (drive_train_cnt != local_train_cnt) or (drive_test_cnt != local_test_cnt)\n","print(\"Need sync:\", need_sync)\n","\n","if need_sync:\n","    rsync_dir(drive_train, LOCAL_TRAIN)\n","    rsync_dir(drive_test,  LOCAL_TEST)\n","\n","train_root = LOCAL_TRAIN\n","test_root  = LOCAL_TEST\n","\n","local_train_cnt2 = count_images(train_root)\n","local_test_cnt2  = count_images(test_root)\n","\n","print(\"\\n=== READY ===\")\n","print(\"train_root =\", train_root, \"| images:\", local_train_cnt2)\n","print(\"test_root  =\", test_root,  \"| images:\", local_test_cnt2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8xYUT2Z5A3un","executionInfo":{"status":"ok","timestamp":1765746417417,"user_tz":360,"elapsed":641044,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}},"outputId":"1b7e89a4-b571-4205-ee3f-1482de4c119e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Drive train images: 20371 | Local train images: 15943\n","Drive test  images: 251  | Local test  images: 0\n","Need sync: True\n","Running: rsync -a --delete --info=progress2 \"/content/drive/My Drive/CS441/Final/garbage_dataset/train/\" /content/garbage_train/\n","Running: rsync -a --delete --info=progress2 \"/content/drive/My Drive/CS441/Final/garbage_dataset/test/\" /content/garbage_test/\n","\n","=== READY ===\n","train_root = /content/garbage_train | images: 20371\n","test_root  = /content/garbage_test | images: 251\n"]}]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Device:\", device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TKlmXnejjgST","executionInfo":{"status":"ok","timestamp":1765746649161,"user_tz":360,"elapsed":12,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}},"outputId":"42f8ec32-68ab-46b1-a380-74f9d9fb5b1f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"O8ddF2qP2jnD","executionInfo":{"status":"ok","timestamp":1765746651172,"user_tz":360,"elapsed":20,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}}},"outputs":[],"source":["class GarbageFineDataset(Dataset):\n","    def __init__(self, root_dir, transform=None,\n","                 fine_class_to_idx=None, big_class_to_idx=None,\n","                 strict=False,\n","                 extensions=('.jpg', '.jpeg', '.png', '.bmp', '.webp', '.heic', '.heif')):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.extensions = extensions\n","        self.strict = strict\n","\n","        self.fine_class_to_idx = {} if fine_class_to_idx is None else dict(fine_class_to_idx)\n","        self.big_class_to_idx  = {} if big_class_to_idx  is None else dict(big_class_to_idx)\n","\n","        self.samples = []  # (image_path, fine_idx, big_idx)\n","\n","        big_names = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n","        for big_name in big_names:\n","            big_path = os.path.join(root_dir, big_name)\n","\n","            if big_name in self.big_class_to_idx:\n","                big_idx = self.big_class_to_idx[big_name]\n","            else:\n","                if self.strict:\n","                    raise ValueError(f\"[STRICT] test/show train/not exist big category: {big_name}\")\n","                big_idx = len(self.big_class_to_idx)\n","                self.big_class_to_idx[big_name] = big_idx\n","\n","            fine_names = sorted([d for d in os.listdir(big_path) if os.path.isdir(os.path.join(big_path, d))])\n","            for fine_name in fine_names:\n","                fine_path = os.path.join(big_path, fine_name)\n","\n","                fine_full_name = f\"{big_name}/{fine_name}\"\n","\n","                if fine_full_name in self.fine_class_to_idx:\n","                    fine_idx = self.fine_class_to_idx[fine_full_name]\n","                else:\n","                    if self.strict:\n","                        raise ValueError(f\"[[STRICT] test/show train/not exist small category: {fine_full_name}\")\n","                    fine_idx = len(self.fine_class_to_idx)\n","                    self.fine_class_to_idx[fine_full_name] = fine_idx\n","\n","\n","                for fname in os.listdir(fine_path):\n","                    if fname.lower().endswith(self.extensions):\n","                        img_path = os.path.join(fine_path, fname)\n","                        self.samples.append((img_path, fine_idx, big_idx))\n","\n","        print(f\"[{root_dir}] samples={len(self.samples)}, big={len(self.big_class_to_idx)}, fine={len(self.fine_class_to_idx)}\")\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        img_path, fine_idx, big_idx = self.samples[idx]\n","        img = Image.open(img_path).convert(\"RGB\")\n","        if self.transform:\n","            img = self.transform(img)\n","        return img, fine_idx, big_idx\n","\n","    @property\n","    def fine_idx_to_name(self):\n","        return {v: k for k, v in self.fine_class_to_idx.items()}\n","\n","    @property\n","    def big_idx_to_name(self):\n","        return {v: k for k, v in self.big_class_to_idx.items()}\n","\n","    @property\n","    def fine_to_big(self):\n","        # fine_idx -> big_idx\n","        mapping = {}\n","        for _, f_idx, b_idx in self.samples:\n","            mapping[f_idx] = b_idx\n","        return mapping"]},{"cell_type":"code","source":["img_size = 224\n","batch_size = 32\n","num_workers = 4\n","\n","train_transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.RandomResizedCrop(img_size),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                         std=[0.229, 0.224, 0.225]),\n","])\n","\n","val_test_transform = transforms.Compose([\n","    transforms.Resize((img_size, img_size)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                         std=[0.229, 0.224, 0.225]),\n","])\n","\n","# 0.9 / 0.1\n","full_train_dataset = GarbageFineDataset(train_root, transform=train_transform)\n","\n","full_train_for_val = GarbageFineDataset(\n","    train_root,\n","    transform=val_test_transform,\n","    fine_class_to_idx=full_train_dataset.fine_class_to_idx,\n","    big_class_to_idx=full_train_dataset.big_class_to_idx,\n","    strict=True\n",")\n","\n","seed = 42\n","g = torch.Generator().manual_seed(seed)\n","\n","N = len(full_train_dataset)\n","n_train = int(0.9 * N)\n","n_val = N - n_train\n","train_subset, val_subset = random_split(range(N), [n_train, n_val], generator=g)\n","\n","\n","class IndexSubset(Dataset):\n","    def __init__(self, base_dataset, indices):\n","        self.base = base_dataset\n","        self.indices = list(indices)\n","    def __len__(self):\n","        return len(self.indices)\n","    def __getitem__(self, i):\n","        return self.base[self.indices[i]]\n","\n","train_dataset = IndexSubset(full_train_dataset, train_subset)\n","val_dataset   = IndexSubset(full_train_for_val, val_subset)\n","\n","test_dataset = GarbageFineDataset(\n","    test_root,\n","    transform=val_test_transform,\n","    fine_class_to_idx=full_train_dataset.fine_class_to_idx,\n","    big_class_to_idx=full_train_dataset.big_class_to_idx,\n","    strict=True\n",")\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=num_workers)\n","val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=num_workers)\n","\n","fine_idx_to_name = full_train_dataset.fine_idx_to_name\n","big_idx_to_name  = full_train_dataset.big_idx_to_name\n","fine_to_big      = full_train_dataset.fine_to_big\n","\n","print(\"Big classes:\", big_idx_to_name)\n","print(\"Fine classes (examples):\", list(fine_idx_to_name.items())[:11])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mWkxRwuK_J4l","executionInfo":{"status":"ok","timestamp":1765746653210,"user_tz":360,"elapsed":118,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}},"outputId":"de18df72-ed54-4c53-aea4-f5324de90c3b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[/content/garbage_train] samples=20371, big=4, fine=16\n","[/content/garbage_train] samples=20371, big=4, fine=16\n","[/content/garbage_test] samples=251, big=4, fine=16\n","Big classes: {0: 'recycling', 1: 'special', 2: 'trash', 3: 'yard_waste'}\n","Fine classes (examples): [(0, 'recycling/cardboard'), (1, 'recycling/glass'), (2, 'recycling/metal'), (3, 'recycling/paper'), (4, 'recycling/plastic'), (5, 'special/battery'), (6, 'special/cables'), (7, 'special/keyboard'), (8, 'special/mouse'), (9, 'special/tire'), (10, 'trash/biological')]\n"]}]},{"cell_type":"markdown","source":["# 1. SVM"],"metadata":{"id":"aMI6h4VYtADL"}},{"cell_type":"code","source":["svm_transform = transforms.Compose([\n","    transforms.Resize((96, 96)),\n","    transforms.ToTensor(),\n","])\n","svm_full_train = GarbageFineDataset(\n","    train_root,\n","    transform=svm_transform,\n","    fine_class_to_idx=full_train_dataset.fine_class_to_idx,\n","    big_class_to_idx=full_train_dataset.big_class_to_idx,\n","    strict=True\n",")\n","svm_train_dataset = IndexSubset(svm_full_train, train_subset)\n","svm_val_dataset   = IndexSubset(svm_full_train, val_subset)\n","\n","svm_test_dataset = GarbageFineDataset(\n","    test_root,\n","    transform=svm_transform,\n","    fine_class_to_idx=full_train_dataset.fine_class_to_idx,\n","    big_class_to_idx=full_train_dataset.big_class_to_idx,\n","    strict=True\n",")\n","\n","svm_batch_size = 64\n","svm_train_loader = DataLoader(svm_train_dataset, batch_size=svm_batch_size, shuffle=False, num_workers=num_workers)\n","svm_val_loader   = DataLoader(svm_val_dataset,   batch_size=svm_batch_size, shuffle=False, num_workers=num_workers)\n","svm_test_loader  = DataLoader(svm_test_dataset,  batch_size=svm_batch_size, shuffle=False, num_workers=num_workers)\n","\n","\n","\n","@torch.no_grad()\n","def extract_flat_features(loader):\n","    feats, labels = [], []\n","    for images, fine_labels, _ in loader:\n","        flat = images.view(images.size(0), -1)\n","        feats.append(flat.cpu().numpy())\n","        labels.append(fine_labels.numpy())\n","    return np.concatenate(feats), np.concatenate(labels)\n","\n","\n","svm_train_feats, svm_train_labels = extract_flat_features(svm_train_loader)\n","svm_val_feats, svm_val_labels = extract_flat_features(svm_val_loader)\n","svm_test_feats, svm_test_labels = extract_flat_features(svm_test_loader)\n","\n","scaler = StandardScaler()\n","pca_components = 256\n","svm_train_feats = scaler.fit_transform(svm_train_feats)\n","svm_val_feats = scaler.transform(svm_val_feats)\n","svm_test_feats = scaler.transform(svm_test_feats)\n","\n","pca = PCA(n_components=pca_components, random_state=42)\n","svm_train_feats = pca.fit_transform(svm_train_feats)\n","svm_val_feats = pca.transform(svm_val_feats)\n","svm_test_feats = pca.transform(svm_test_feats)\n","\n","svm_clf = SGDClassifier(\n","    loss=\"hinge\",\n","    alpha=1e-5,\n","    max_iter=500,\n","    tol=1e-4,\n","    n_jobs=-1,\n","    class_weight=\"balanced\",\n","    random_state=42,\n","    verbose=0,\n",")\n","t0 = time.time()\n","svm_clf.fit(svm_train_feats, svm_train_labels)\n","svm_val_pred = svm_clf.predict(svm_val_feats)\n","svm_test_pred = svm_clf.predict(svm_test_feats)\n","\n","svm_val_acc = (svm_val_pred == svm_val_labels).mean()\n","svm_test_acc = (svm_test_pred == svm_test_labels).mean()\n","\n","print(f\"[SVM] Val acc={svm_val_acc:.4f} | Test acc={svm_test_acc:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"artEgIyD_CPP","executionInfo":{"status":"ok","timestamp":1765746838723,"user_tz":360,"elapsed":67292,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}},"outputId":"e72196d4-7cd8-4af8-ae7a-4102f3020e7f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[/content/garbage_train] samples=20371, big=4, fine=16\n","[/content/garbage_test] samples=251, big=4, fine=16\n","[SVM] Val acc=0.3734 | Test acc=0.1195\n"]}]},{"cell_type":"markdown","source":["# 2. CNN"],"metadata":{"id":"_dEdP9jytviS"}},{"cell_type":"code","source":["num_fine_classes = len(full_train_dataset.fine_class_to_idx)\n","class SimpleCNN(nn.Module):\n","    def __init__(self, num_classes):\n","        super().__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2),\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(2),\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.AdaptiveAvgPool2d((1, 1)),\n","        )\n","        self.classifier = nn.Linear(128, num_classes)\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = torch.flatten(x, 1)\n","        return self.classifier(x)\n","\n","\n","def train_cnn_one_epoch(model, loader, optimizer, criterion, device):\n","    model.train()\n","    total_loss, correct, total = 0.0, 0, 0\n","    for images, fine_labels, _ in loader:\n","        images = images.to(device)\n","        fine_labels = fine_labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, fine_labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item() * images.size(0)\n","        pred = outputs.argmax(dim=1)\n","        correct += (pred == fine_labels).sum().item()\n","        total += fine_labels.size(0)\n","    return total_loss / total, correct / total\n","\n","\n","@torch.no_grad()\n","def eval_cnn_one_epoch(model, loader, criterion, device):\n","    model.eval()\n","    total_loss, correct, total = 0.0, 0, 0\n","    for images, fine_labels, _ in loader:\n","        images = images.to(device)\n","        fine_labels = fine_labels.to(device)\n","\n","        outputs = model(images)\n","        loss = criterion(outputs, fine_labels)\n","\n","        total_loss += loss.item() * images.size(0)\n","        pred = outputs.argmax(dim=1)\n","        correct += (pred == fine_labels).sum().item()\n","        total += fine_labels.size(0)\n","    return total_loss / total, correct / total\n","\n","\n","cnn_model = SimpleCNN(num_fine_classes).to(device)\n","cnn_criterion = nn.CrossEntropyLoss()\n","cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=1e-3)\n","cnn_epochs = 10\n","cnn_best_acc = 0.0\n","cnn_best_state = None\n","\n","for epoch in range(cnn_epochs):\n","    tr_loss, tr_acc = train_cnn_one_epoch(cnn_model, train_loader, cnn_optimizer, cnn_criterion, device)\n","    va_loss, va_acc = eval_cnn_one_epoch(cnn_model, val_loader, cnn_criterion, device)\n","    print(f\"[CNN] Epoch {epoch+1}/{cnn_epochs} | Train loss={tr_loss:.4f}, acc={tr_acc:.4f} | Val loss={va_loss:.4f}, acc={va_acc:.4f}\")\n","    if va_acc > cnn_best_acc:\n","        cnn_best_acc = va_acc\n","        cnn_best_state = {k: v.cpu() for k, v in cnn_model.state_dict().items()}\n","\n","if cnn_best_acc > 0:\n","    cnn_model.load_state_dict(cnn_best_state)\n","    cnn_model = cnn_model.to(device)\n","print(\"[CNN] Best val acc:\", cnn_best_acc)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aw909vbT9_vu","executionInfo":{"status":"ok","timestamp":1765747331134,"user_tz":360,"elapsed":443599,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}},"outputId":"8381a462-2866-42f2-d66d-748744e3ded3"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[CNN] Epoch 1/10 | Train loss=1.8990, acc=0.3824 | Val loss=1.7342, acc=0.4642\n","[CNN] Epoch 2/10 | Train loss=1.7634, acc=0.4262 | Val loss=1.6741, acc=0.4769\n","[CNN] Epoch 3/10 | Train loss=1.7054, acc=0.4435 | Val loss=1.6038, acc=0.5020\n","[CNN] Epoch 4/10 | Train loss=1.6543, acc=0.4612 | Val loss=1.5930, acc=0.5079\n","[CNN] Epoch 5/10 | Train loss=1.6063, acc=0.4768 | Val loss=1.6048, acc=0.4902\n","[CNN] Epoch 6/10 | Train loss=1.5679, acc=0.4903 | Val loss=1.4820, acc=0.5206\n","[CNN] Epoch 7/10 | Train loss=1.5332, acc=0.5009 | Val loss=1.4561, acc=0.5402\n","[CNN] Epoch 8/10 | Train loss=1.5196, acc=0.5062 | Val loss=1.3990, acc=0.5559\n","[CNN] Epoch 9/10 | Train loss=1.4680, acc=0.5202 | Val loss=1.4069, acc=0.5648\n","[CNN] Epoch 10/10 | Train loss=1.4526, acc=0.5232 | Val loss=1.2763, acc=0.5942\n","[CNN] Best val acc: 0.5942100098135427\n"]}]},{"cell_type":"markdown","source":["# 3. Fine tuned Resnet 18"],"metadata":{"id":"0WgRForQuO1w"}},{"cell_type":"markdown","source":["Stage 1: Freeze the backbone and only train the last layer"],"metadata":{"id":"Jnt2UfYm2xaw"}},{"cell_type":"code","source":["weights = models.ResNet18_Weights.IMAGENET1K_V1\n","model = models.resnet18(weights=weights)\n","\n","for p in model.parameters():\n","    p.requires_grad = False\n","\n","num_features = model.fc.in_features\n","num_fine_classes = len(full_train_dataset.fine_class_to_idx)\n","model.fc = nn.Linear(num_features, num_fine_classes)\n","\n","model = model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)"],"metadata":{"id":"nSLSqBMTDiJc","executionInfo":{"status":"ok","timestamp":1765747344209,"user_tz":360,"elapsed":891,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f4c6af09-732f-42ac-90e8-46923e826913"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 44.7M/44.7M [00:00<00:00, 97.2MB/s]\n"]}]},{"cell_type":"code","source":["def train_one_epoch(model, loader, optimizer, criterion, device):\n","    model.train()\n","    total_loss, correct, total = 0.0, 0, 0\n","\n","    for images, fine_labels, _ in loader:\n","        images = images.to(device)\n","        fine_labels = fine_labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, fine_labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item() * images.size(0)\n","        pred = outputs.argmax(dim=1)\n","        correct += (pred == fine_labels).sum().item()\n","        total += fine_labels.size(0)\n","\n","    return total_loss / total, correct / total\n","\n","@torch.no_grad()\n","def eval_one_epoch(model, loader, criterion, device):\n","    model.eval()\n","    total_loss, correct, total = 0.0, 0, 0\n","\n","    for images, fine_labels, _ in loader:\n","        images = images.to(device)\n","        fine_labels = fine_labels.to(device)\n","\n","        outputs = model(images)\n","        loss = criterion(outputs, fine_labels)\n","\n","        total_loss += loss.item() * images.size(0)\n","        pred = outputs.argmax(dim=1)\n","        correct += (pred == fine_labels).sum().item()\n","        total += fine_labels.size(0)\n","\n","    return total_loss / total, correct / total"],"metadata":{"id":"fFmNFQRiDtTX","executionInfo":{"status":"ok","timestamp":1765747346219,"user_tz":360,"elapsed":14,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["num_epochs = 10\n","best_val_acc = 0.0\n","best_state = None\n","\n","for epoch in range(num_epochs):\n","    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n","    va_loss, va_acc = eval_one_epoch(model, val_loader, criterion, device)\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs} | \"\n","          f\"Train loss={tr_loss:.4f}, acc={tr_acc:.4f} | \"\n","          f\"Val loss={va_loss:.4f}, acc={va_acc:.4f}\")\n","\n","    if va_acc > best_val_acc:\n","        best_val_acc = va_acc\n","        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n","\n","if best_state is not None:\n","    model.load_state_dict(best_state)\n","    model = model.to(device)\n","\n","print(\"Best val acc:\", best_val_acc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VTz81IARD0ee","executionInfo":{"status":"ok","timestamp":1765747790019,"user_tz":360,"elapsed":431892,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}},"outputId":"fd3069b2-e74a-4b3b-a101-f6c5e13b987a"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10 | Train loss=0.8239, acc=0.7554 | Val loss=0.3957, acc=0.8817\n","Epoch 2/10 | Train loss=0.5305, acc=0.8326 | Val loss=0.3148, acc=0.9033\n","Epoch 3/10 | Train loss=0.4863, acc=0.8439 | Val loss=0.3107, acc=0.9028\n","Epoch 4/10 | Train loss=0.4674, acc=0.8491 | Val loss=0.2777, acc=0.9136\n","Epoch 5/10 | Train loss=0.4524, acc=0.8536 | Val loss=0.2988, acc=0.9048\n","Epoch 6/10 | Train loss=0.4460, acc=0.8553 | Val loss=0.2949, acc=0.9063\n","Epoch 7/10 | Train loss=0.4397, acc=0.8573 | Val loss=0.2955, acc=0.9097\n","Epoch 8/10 | Train loss=0.4334, acc=0.8598 | Val loss=0.2877, acc=0.9112\n","Epoch 9/10 | Train loss=0.4326, acc=0.8582 | Val loss=0.2906, acc=0.9024\n","Epoch 10/10 | Train loss=0.4291, acc=0.8612 | Val loss=0.3078, acc=0.9048\n","Best val acc: 0.9136408243375859\n"]}]},{"cell_type":"markdown","source":["Stage 2: Unfreeze layer 4 and fc"],"metadata":{"id":"eoyQ2Bsn3MyT"}},{"cell_type":"code","source":["for name, p in model.named_parameters():\n","    p.requires_grad = name.startswith(\"layer4\") or name.startswith(\"fc\")\n","\n","optimizer = optim.AdamW([\n","    {\"params\": model.layer4.parameters(), \"lr\": 1e-4},\n","    {\"params\": model.fc.parameters(),     \"lr\": 5e-4},\n","], weight_decay=1e-4)\n","\n","scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5)\n","\n","num_epochs_ft = 5\n","best_val_acc_ft = 0.0\n","best_state_ft = None\n","\n","for epoch in range(num_epochs_ft):\n","    tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n","    va_loss, va_acc = eval_one_epoch(model, val_loader, criterion, device)\n","\n","    print(f\"[FT] Epoch {epoch+1}/{num_epochs_ft} | \"\n","          f\"Train loss={tr_loss:.4f}, acc={tr_acc:.4f} | \"\n","          f\"Val loss={va_loss:.4f}, acc={va_acc:.4f}\", flush=True)\n","\n","    scheduler.step()\n","\n","    if va_acc > best_val_acc_ft:\n","        best_val_acc_ft = va_acc\n","        best_state_ft = {k: v.cpu() for k, v in model.state_dict().items()}\n","\n","if best_state_ft is not None:\n","    model.load_state_dict(best_state_ft)\n","    model = model.to(device)\n","\n","print(\"Best FT val acc:\", best_val_acc_ft)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lsEyNa0c6sEJ","executionInfo":{"status":"ok","timestamp":1765748099306,"user_tz":360,"elapsed":217631,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}},"outputId":"aaf5c69b-7650-43d5-b055-7143d5583a09"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[FT] Epoch 1/5 | Train loss=0.4423, acc=0.8595 | Val loss=0.2472, acc=0.9249\n","[FT] Epoch 2/5 | Train loss=0.3057, acc=0.9002 | Val loss=0.2236, acc=0.9338\n","[FT] Epoch 3/5 | Train loss=0.2463, acc=0.9201 | Val loss=0.1948, acc=0.9460\n","[FT] Epoch 4/5 | Train loss=0.1929, acc=0.9364 | Val loss=0.1782, acc=0.9460\n","[FT] Epoch 5/5 | Train loss=0.1688, acc=0.9443 | Val loss=0.1680, acc=0.9509\n","Best FT val acc: 0.9509322865554465\n"]}]},{"cell_type":"markdown","source":["# 4. Fine tuned Mobile Net V3"],"metadata":{"id":"CjMnTZEiuvj-"}},{"cell_type":"code","source":["mb_weights = models.MobileNet_V3_Large_Weights.IMAGENET1K_V1\n","mobile_model = models.mobilenet_v3_large(weights=mb_weights)\n","mobile_model.classifier[-1] = nn.Linear(mobile_model.classifier[-1].in_features, num_fine_classes)\n","\n","for p in mobile_model.features.parameters():\n","    p.requires_grad = False\n","\n","mobile_model = mobile_model.to(device)\n","mobile_criterion = nn.CrossEntropyLoss()\n","mobile_optimizer = optim.Adam(mobile_model.classifier.parameters(), lr=1e-3)\n","mobile_epochs = 10\n","mobile_best_acc = 0.0\n","mobile_best_state = None\n","\n","for epoch in range(mobile_epochs):\n","    tr_loss, tr_acc = train_one_epoch(mobile_model, train_loader, mobile_optimizer, mobile_criterion, device)\n","    va_loss, va_acc = eval_one_epoch(mobile_model, val_loader, mobile_criterion, device)\n","    print(f\"[MobileNet] Epoch {epoch+1}/{mobile_epochs} | Train loss={tr_loss:.4f}, acc={tr_acc:.4f} | Val loss={va_loss:.4f}, acc={va_acc:.4f}\")\n","    if va_acc > mobile_best_acc:\n","        mobile_best_acc = va_acc\n","        mobile_best_state = {k: v.cpu() for k, v in mobile_model.state_dict().items()}\n","\n","if mobile_best_state is not None:\n","    mobile_model.load_state_dict(mobile_best_state)\n","    mobile_model = mobile_model.to(device)\n","print(\"[MobileNet] Best val acc:\", mobile_best_acc)\n","\n","# fine-tune: unfreeze backbone with smaller LR for a few epochs\n","for p in mobile_model.features.parameters():\n","    p.requires_grad = True\n","\n","mobile_ft_optimizer = optim.AdamW([\n","    {\"params\": mobile_model.features.parameters(), \"lr\": 3e-5, \"weight_decay\": 1e-4},\n","    {\"params\": mobile_model.classifier.parameters(), \"lr\": 3e-4, \"weight_decay\": 1e-4},\n","])\n","mobile_ft_epochs = 5\n","mobile_ft_best_acc = 0.0\n","mobile_ft_best_state = None\n","\n","for epoch in range(mobile_ft_epochs):\n","    tr_loss, tr_acc = train_one_epoch(mobile_model, train_loader, mobile_ft_optimizer, mobile_criterion, device)\n","    va_loss, va_acc = eval_one_epoch(mobile_model, val_loader, mobile_criterion, device)\n","    print(f\"[MobileNet-FT] Epoch {epoch+1}/{mobile_ft_epochs} | Train loss={tr_loss:.4f}, acc={tr_acc:.4f} | Val loss={va_loss:.4f}, acc={va_acc:.4f}\")\n","    if va_acc > mobile_ft_best_acc:\n","        mobile_ft_best_acc = va_acc\n","        mobile_ft_best_state = {k: v.cpu() for k, v in mobile_model.state_dict().items()}\n","\n","if mobile_ft_best_state is not None:\n","    mobile_model.load_state_dict(mobile_ft_best_state)\n","    mobile_model = mobile_model.to(device)\n","print(\"[MobileNet-FT] Best val acc:\", mobile_ft_best_acc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lxc-CSYLD2Df","executionInfo":{"status":"ok","timestamp":1765748843210,"user_tz":360,"elapsed":651261,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}},"outputId":"3514182e-ecf0-4d73-a7a5-848204bf349c"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 21.1M/21.1M [00:00<00:00, 70.7MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["[MobileNet] Epoch 1/10 | Train loss=0.6210, acc=0.8090 | Val loss=0.3032, acc=0.9053\n","[MobileNet] Epoch 2/10 | Train loss=0.4433, acc=0.8573 | Val loss=0.2660, acc=0.9171\n","[MobileNet] Epoch 3/10 | Train loss=0.4037, acc=0.8709 | Val loss=0.2370, acc=0.9303\n","[MobileNet] Epoch 4/10 | Train loss=0.3640, acc=0.8824 | Val loss=0.2605, acc=0.9190\n","[MobileNet] Epoch 5/10 | Train loss=0.3515, acc=0.8888 | Val loss=0.2331, acc=0.9323\n","[MobileNet] Epoch 6/10 | Train loss=0.3385, acc=0.8909 | Val loss=0.2535, acc=0.9195\n","[MobileNet] Epoch 7/10 | Train loss=0.3201, acc=0.8978 | Val loss=0.2629, acc=0.9195\n","[MobileNet] Epoch 8/10 | Train loss=0.3129, acc=0.8996 | Val loss=0.2550, acc=0.9220\n","[MobileNet] Epoch 9/10 | Train loss=0.3127, acc=0.9023 | Val loss=0.2167, acc=0.9342\n","[MobileNet] Epoch 10/10 | Train loss=0.2988, acc=0.9058 | Val loss=0.2332, acc=0.9347\n","[MobileNet] Best val acc: 0.9347399411187438\n","[MobileNet-FT] Epoch 1/5 | Train loss=0.2264, acc=0.9279 | Val loss=0.1811, acc=0.9534\n","[MobileNet-FT] Epoch 2/5 | Train loss=0.1710, acc=0.9435 | Val loss=0.1731, acc=0.9509\n","[MobileNet-FT] Epoch 3/5 | Train loss=0.1692, acc=0.9464 | Val loss=0.1521, acc=0.9588\n","[MobileNet-FT] Epoch 4/5 | Train loss=0.1400, acc=0.9555 | Val loss=0.1504, acc=0.9598\n","[MobileNet-FT] Epoch 5/5 | Train loss=0.1349, acc=0.9561 | Val loss=0.1582, acc=0.9578\n","[MobileNet-FT] Best val acc: 0.9597644749754661\n"]}]},{"cell_type":"markdown","source":["# 5. Strategies evaluation and comparison"],"metadata":{"id":"2Tu-rwvOu4MG"}},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import classification_report, confusion_matrix\n","from matplotlib.colors import PowerNorm, LogNorm\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","@torch.no_grad()\n","def collect_fine_preds(model, loader, device):\n","    model.eval()\n","    y_true, y_pred = [], []\n","\n","    for images, fine_labels, _ in loader:\n","        images = images.to(device)\n","        outputs = model(images)\n","        pred = outputs.argmax(dim=1).cpu().numpy()\n","\n","        y_true.extend(fine_labels.numpy())\n","        y_pred.extend(pred)\n","\n","    return np.array(y_true), np.array(y_pred)\n","\n","\n","def _plot_confusion_matrix(cm, class_names, title, normalize=False,\n","                           cmap=\"Blues\", use_log_for_counts=True, vmax_percentile=99):\n","\n","    cm_plot = cm.astype(np.float64)\n","\n","    if normalize:\n","        row_sums = cm_plot.sum(axis=1, keepdims=True)\n","        cm_plot = np.divide(cm_plot, row_sums, out=np.zeros_like(cm_plot), where=row_sums != 0) * 100.0\n","\n","    n = len(class_names)\n","\n","    fig_w = min(18, max(10, 0.65 * n))\n","    fig_h = min(18, max(7,  0.60 * n))\n","    plt.figure(figsize=(fig_w, fig_h))\n","\n","    if normalize:\n","\n","        vmax = 100.0\n","        norm = PowerNorm(gamma=0.5, vmin=0.0, vmax=vmax)\n","        im = plt.imshow(cm_plot, interpolation=\"nearest\", cmap=cmap, norm=norm)\n","    else:\n","\n","        nonzero = cm_plot[cm_plot > 0]\n","        vmax = np.percentile(nonzero, vmax_percentile) if nonzero.size else 1.0\n","\n","        if use_log_for_counts:\n","\n","            masked = np.ma.masked_where(cm_plot == 0, cm_plot)\n","            norm = LogNorm(vmin=1, vmax=max(1, vmax))\n","            im = plt.imshow(masked, interpolation=\"nearest\", cmap=cmap, norm=norm)\n","        else:\n","\n","            norm = PowerNorm(gamma=0.5, vmin=0.0, vmax=vmax)\n","            im = plt.imshow(cm_plot, interpolation=\"nearest\", cmap=cmap, norm=norm)\n","\n","    plt.title(title)\n","    plt.colorbar(im, fraction=0.046, pad=0.04)\n","\n","    tick_marks = np.arange(n)\n","    plt.xticks(tick_marks, class_names, rotation=45, ha=\"right\", fontsize=9)\n","    plt.yticks(tick_marks, class_names, fontsize=9)\n","\n","\n","    display_max = np.nanmax(cm_plot) if normalize else (np.nanmax(cm_plot[cm_plot > 0]) if np.any(cm_plot > 0) else 1.0)\n","    thresh = display_max * 0.5\n","\n","    for i in range(n):\n","        for j in range(n):\n","            val = cm_plot[i, j]\n","            if normalize:\n","                text = f\"{val:.1f}\"\n","                show = (val > 0)\n","            else:\n","                text = str(int(cm[i, j]))\n","                show = (cm[i, j] > 0)\n","\n","            if show:\n","                plt.text(j, i, text,\n","                         ha=\"center\", va=\"center\",\n","                         fontsize=8,\n","                         color=\"white\" if val > thresh else \"black\")\n","\n","    plt.ylabel(\"True label\")\n","    plt.xlabel(\"Predicted label\")\n","    plt.tight_layout()\n","    plt.show()\n","\n","\n","def evaluate_report(y_fine_true, y_fine_pred, fine_idx_to_name, fine_to_big, big_idx_to_name, set_name=\"\"):\n","\n","    fine_names = [fine_idx_to_name[i] for i in range(len(fine_idx_to_name))]\n","\n","    print(f\"\\n=== {set_name} Fine-class Report  ===\")\n","    print(classification_report(y_fine_true, y_fine_pred, target_names=fine_names, zero_division=0))\n","\n","    cm_fine = confusion_matrix(y_fine_true, y_fine_pred, labels=np.arange(len(fine_names)))\n","    _plot_confusion_matrix(cm_fine, fine_names, title=f\"{set_name} Fine Confusion Matrix (Counts)\", normalize=False)\n","    _plot_confusion_matrix(cm_fine, fine_names, title=f\"{set_name} Fine Confusion Matrix (Row %)\", normalize=True)\n","\n","    y_big_true = np.array([fine_to_big[int(i)] for i in y_fine_true])\n","    y_big_pred = np.array([fine_to_big[int(i)] for i in y_fine_pred])\n","\n","    big_names = [big_idx_to_name[i] for i in range(len(big_idx_to_name))]\n","\n","    print(f\"\\n=== {set_name} Big-class Report  ===\")\n","    print(classification_report(y_big_true, y_big_pred, target_names=big_names, zero_division=0))\n","\n","    cm_big = confusion_matrix(y_big_true, y_big_pred, labels=np.arange(len(big_names)))\n","    _plot_confusion_matrix(cm_big, big_names, title=f\"{set_name} Big Confusion Matrix (Counts)\", normalize=False)\n","    _plot_confusion_matrix(cm_big, big_names, title=f\"{set_name} Big Confusion Matrix (Row %)\", normalize=True)\n","\n","print(\"\\n######## SVM VAL SET EVAL ########\")\n","evaluate_report(svm_val_labels, svm_val_pred, fine_idx_to_name, fine_to_big, big_idx_to_name, set_name=\"SVM VAL\")\n","\n","print(\"\\n######## SVM TEST SET EVAL ########\")\n","evaluate_report(svm_test_labels, svm_test_pred, fine_idx_to_name, fine_to_big, big_idx_to_name, set_name=\"SVM TEST\")\n","\n","print(\"\\n######## CNN VAL SET EVAL ########\")\n","y_cnn_val_true, y_cnn_val_pred = collect_fine_preds(cnn_model, val_loader, device)\n","evaluate_report(y_cnn_val_true, y_cnn_val_pred, fine_idx_to_name, fine_to_big, big_idx_to_name, set_name=\"CNN VAL\")\n","\n","print(\"\\n######## CNN TEST SET EVAL ########\")\n","y_cnn_test_true, y_cnn_test_pred = collect_fine_preds(cnn_model, test_loader, device)\n","evaluate_report(y_cnn_test_true, y_cnn_test_pred, fine_idx_to_name, fine_to_big, big_idx_to_name, set_name=\"CNN TEST\")\n","\n","y_val_true, y_val_pred = collect_fine_preds(model, val_loader, device)\n","print(\"\\n######## ResNet VAL SET EVAL ########\")\n","evaluate_report(y_val_true, y_val_pred, fine_idx_to_name, fine_to_big, big_idx_to_name, set_name=\"VAL\")\n","\n","y_test_true, y_test_pred = collect_fine_preds(model, test_loader, device)\n","print(\"\\n######## ResNet TEST SET EVAL ########\")\n","evaluate_report(y_test_true, y_test_pred, fine_idx_to_name, fine_to_big, big_idx_to_name, set_name=\"TEST\")\n","\n","print(\"\\n######## MobileNet VAL SET EVAL ########\")\n","y_mb_val_true, y_mb_val_pred = collect_fine_preds(mobile_model, val_loader, device)\n","evaluate_report(y_mb_val_true, y_mb_val_pred, fine_idx_to_name, fine_to_big, big_idx_to_name, set_name=\"MobileNet VAL\")\n","\n","print(\"\\n######## MobileNet TEST SET EVAL ########\")\n","y_mb_test_true, y_mb_test_pred = collect_fine_preds(mobile_model, test_loader, device)\n","evaluate_report(y_mb_test_true, y_mb_test_pred, fine_idx_to_name, fine_to_big, big_idx_to_name, set_name=\"MobileNet TEST\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1O0rbQJ9ybz2RmAVZcMpJOTlE3i8eon9L"},"id":"CIsjngHywpo2","executionInfo":{"status":"ok","timestamp":1765748981288,"user_tz":360,"elapsed":74422,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}},"outputId":"bf3e5b54-334e-4bc0-8991-8ccf163d2b59"},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["# from https://gist.github.com/jonathanagustin/b67b97ef12c53a8dec27b343dca4abba\n","# install can take a minute\n","\n","import os\n","# @title Convert Notebook to PDF. Save Notebook to given directory\n","NOTEBOOKS_DIR = \"/content/drive/My Drive/CS441/Final\" # @param {type:\"string\"}\n","NOTEBOOK_NAME = \"Imagenet.ipynb\" # @param {type:\"string\"}\n","#------------------------------------------------------------------------------#\n","from google.colab import drive\n","drive.mount(\"/content/drive/\", force_remount=True)\n","NOTEBOOK_PATH = f\"{NOTEBOOKS_DIR}/{NOTEBOOK_NAME}\"\n","assert os.path.exists(NOTEBOOK_PATH), f\"NOTEBOOK NOT FOUND: {NOTEBOOK_PATH}\"\n","!apt install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic > /dev/null 2>&1\n","!jupyter nbconvert \"$NOTEBOOK_PATH\" --to pdf > /dev/null 2>&1\n","NOTEBOOK_PDF = NOTEBOOK_PATH.rsplit('.', 1)[0] + '.pdf'\n","assert os.path.exists(NOTEBOOK_PDF), f\"ERROR MAKING PDF: {NOTEBOOK_PDF}\"\n","print(f\"PDF CREATED: {NOTEBOOK_PDF}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AavJeRSwv32f","executionInfo":{"status":"ok","timestamp":1765749065366,"user_tz":360,"elapsed":65252,"user":{"displayName":"Yuyang Liu","userId":"04053806102587776134"}},"outputId":"7bfd96a8-5491-4418-fe8b-082853377d48"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","PDF CREATED: /content/drive/My Drive/CS441/Final/Imagenet.pdf\n"]}]}]}